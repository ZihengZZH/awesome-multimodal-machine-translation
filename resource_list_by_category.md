**Resource List**

> [pdf]: paper PDF online link  
> [repo]: paper PDF repo link  
> [github]: github link  
> [web]: website link

- [Paper by category](#paper-by-category)
    - [Multimodal Machine Translation](#multimodal-machine-translation)
    - [Multimodal Language Models](#multimodal-language-models)
    - [Neural Machine Translation](#neural-machine-translation)
- [Datasets](#datasets)
- [Metrics](#metrics)
- [Tutorials](#tutorials)

## Paper by category

#### Multimodal Machine Translation

| _Year_ | <div style="width:120px">_Authors_</div> | <div style="width:120px">_Conf._</div> | _Title_ | _Links_ |
| :----: | ---------------------------------------- | -------------------------------------- | ------- | ------- |
| 2016.08| Huang et al. | WMT'16 | Attention-based Multimodal Neural Machine Translation | [[pdf](https://www.aclweb.org/anthology/W16-2360)] [[repo](paper/huang2016attention.pdf)] |
| 2016.08| Caglayan et al. | WMT'16 | Does Multimodality Help Human and Machine for Translation and Image Captioning? | [[pdf](https://arxiv.org/pdf/1605.09186.pdf)]  [[repo](paper/caglayan2016multimodality.pdf)] |
| 2016.09| Caglayan et al. | arXiv | Multimodal Attention for Neural Machine Translation | [[pdf](https://arxiv.org/pdf/1609.03976.pdf)] [[repo](paper/caglayan2016multimodal.pdf)] |
| 2017.02| Calixto et al. | arXiv | Doubly-Attentive Decoder for Multi-modal Neural Machine Translation | [[pdf](https://arxiv.org/pdf/1702.01287.pdf)] [[repo](paper/calixto2017doubly.pdf)] [[github](https://github.com/iacercalixto/MultimodalNMT)] |
| 2017.03| Delbrouck et al. | ICLR'17 | Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation | [[pdf](https://arxiv.org/pdf/1703.08084.pdf)] [[repo](paper/delbrouck2017multimodal.pdf)] |
| 2017.05| Chen et al. | arXiv | A Teacher-Student Framework for Zero-Resource Neural Machine Translation | [[pdf](https://arxiv.org/pdf/1705.00753.pdf)] [[repo](paper/chen2017teacher.pdf)] |
| 2017.06| Lala et al. | PBML'17 | Unraveling the Contribution of Image Captioning and Neural Machine Translation for Multimodal Machine Translation | [[pdf](https://ufal.mff.cuni.cz/pbml/108/art-lala-madhyastha-wang-specia.pdf)] [[repo](paper/lala2017unraveling.pdf)] |
| 2017.07| Elliott et al. | arXiv | Imagination improves Multimodal Translation | [[pdf](https://arxiv.org/pdf/1705.04350.pdf)] [[repo](paper/elliott2017imagination.pdf)] |
| 2017.07| Nakayama et al. | arXiv | Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot | [[pdf](https://arxiv.org/pdf/1611.04503.pdf)] [[repo](paper/nakayama2017zeroresource.pdf)] |
| 2017.08| Libovicky et al. | ACL'17 | Attention Strategies for Multi-Source Sequence-to-Sequence Learning | [[pdf](https://aclweb.org/anthology/P17-2031)] [[repo](paper/libovicky2017attention.pdf)] |
| 2017.09| Calixto et al. | EMNLP'17 | Incorporating Global Visual Features into Attention-Based Neural Machine Translation | [[pdf](https://www.aclweb.org/anthology/D17-1105)] [[repo](paper/calixto2017incorporating.pdf)] |
| 2017.10| Elliott et al. | WMT'17 | Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description | [[pdf](https://arxiv.org/abs/1710.07177)] [[repo](paper/elliott2017findings.pdf)] |
| 2018.05| Qian et al. | arXiv | Multimodal Machine Translation with Reinforcement Learning | [[pdf](https://arxiv.org/pdf/1805.02356.pdf)] [[repo](paper/qian2018multimodal.pdf)] |
| 2018.08| Zhou et al. | ACL'18 | A Visual Attention Grounding Neural Model for Multimodal Machine Translation | [[pdf](https://www.aclweb.org/anthology/D18-1400)] [[repo](paper/zhou2018visual.pdf)] |
| 2018.10| Barrault et al. | WMT'18 | Findings of the Third Shared Task on Multimodal Machine Translation | [[pdf](http://statmt.org/wmt18/pdf/WMT029.pdf)] [[repo](paper/barrault2018findings.pdf)] |
| 2018.10| Caglayan et al. | WMT'18 | LIUM-CVC Submissions for WMT18 Multimodal Translation Task | [[pdf](http://statmt.org/wmt18/pdf/WMT065.pdf)] [[repo](paper/caglayan2018LIUM-CVC.pdf)] |
| 2018.10| Gronroos et al. | WMT'18 | The MeMAD Submission to the WMT18 Multimodal Translation Task | [[pdf](http://statmt.org/wmt18/pdf/WMT066.pdf)] [[repo](paper/gronroos2018MeMAD.pdf)] | 
| 2018.10| Gwinnup et al. | WMT'18 | The AFRL-Ohio State WMT18 Multimodal System: Combining Visual with Traditional | [[pdf](http://statmt.org/wmt18/pdf/WMT067.pdf)] [[repo](paper/gwinnup2018AFRL-Ohio.pdf)] |
| 2018.10| Helcl et al. | WMT'18 | CUNI System for the WMT18 Multimodal Translation Task | [[pdf](http://statmt.org/wmt18/pdf/WMT068.pdf)] [[repo](paper/helcl2018CUNI.pdf)] | 
| 2018.10| Lala et al. | WMT'18 | Sheffield Submissions for WMT18 Multimodal Translation Shared Task | [[pdf](http://statmt.org/wmt18/pdf/WMT069.pdf)] [[repo](paper/lala2018sheffield.pdf)] |
| 2018.10| Zheng et al. | WMT'18 | Ensemble Sequence Level Training for Multimodal MT: OSU-Baidu WMT18 Multimodal Translation System Report | [[pdf](http://statmt.org/wmt18/pdf/WMT070.pdf)] [[repo](paper/zheng2018ensemble.pdf)] |
| 2018.10| Delbrouck et al. | WMT'18 | UMONS Submission for WMT18 Multimodal Translation Task | [[pdf](http://statmt.org/wmt18/pdf/WMT071.pdf)] [[repo](paper/delbrouck2018UMONS.pdf)] [[github](https://github.com/jbdel/WMT18_MNMT)] |
| 2018.10| Libovicky et al. | WMT'18 | Input Combination Strategies for Multi-Source Transformer Decoder | [[pdf](https://www.aclweb.org/anthology/W18-6326)] [[repo](paper/libovicky2018input.pdf)] |
| 2018.10| Shin et al. | WMT'18 | Multi-encoder Transformer Network for Automatic Post-Editing | [[pdf](https://www.aclweb.org/anthology/W18-6470)] [[repo](paper/shin2018multi.pdf)] |
| 2019.04| Calixto et al. | Springer | An Error Analysis for Image-based Multi-modal Neural Machine Translation | [[pdf](https://link.springer.com/content/pdf/10.1007%2Fs10590-019-09226-9.pdf)] [[repo](paper/calixto2019error.pdf)] |
| 2019.04| Hirasawa et al. | arXiv | Multimodal Machine Translation with Embedding Prediction | [[pdf](https://arxiv.org/pdf/1904.00639.pdf)] [[repo](paper/hirasawa2019multimodal.pdf)] [[github](https://github.com/toshohirasawa/nmtpytorch-emb-pred)] |
| 2019.06| Caglayan et al. | NAACL-HLT'19 | Probing the Need for Visual Context in Multimodal Machine Translation | [[pdf](https://arxiv.org/pdf/1903.08678.pdf)] [[repo](paper/caglayan2019probing.pdf)] |
| 2019.06| Su et al. | CVPR'19 | Unsupervised Multi-modal Neural Machine Translation | [[pdf](https://arxiv.org/pdf/1811.11365.pdf)] [[repo](paper/su2019unsupervised.pdf)] |
| 2019.06| Chen et al. | IJCAI'19 | From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots | [[pdf](https://arxiv.org/pdf/1906.00872.pdf)] [[repo](paper/chen2019from.pdf)] |
| 2019.07| Calixto et al. | ACL'19 | Latent Variable Model for Multi-modal Translation | [[pdf](https://www.aclweb.org/anthology/P19-1642)] [[repo](paper/calixto2019latent.pdf)] |
| 2019.07| Ive et al. | ACL'19 | Distilling Translations with Visual Awareness | [[pdf](https://arxiv.org/pdf/1906.07701.pdf)] [[repo](paper/ive2019distilling.pdf)] [[github](https://github.com/ImperialNLP/MMT-Delib)] |
| 2019.07| Hirasawa et al. | ACL'19 | Debiasing Word Embedding Improves Multimodal Machine Translation | [[pdf](https://arxiv.org/pdf/1905.10464.pdf)] [[repo](paper/hirasawa2019debiasing.pdf)] |
| 2019.07| Mogadala et al. | arXiv | Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods | [[pdf](https://arxiv.org/pdf/1907.09358.pdf)] [[repo](paper/mogadala2019trends.pdf)] |
| 2020.02| Yang et al. | AAAI'20 | Visual Agreement Regularized Training for Multi-Modal Machine Translation | [[pdf](https://arxiv.org/pdf/1912.12014.pdf)] [[repo](paper/yang2020visual.pdf)] |
| 2020.07| Huang et al. | ACL'20 | Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting | [[pdf](https://arxiv.org/pdf/2005.03119.pdf)] [[repo](paper/huang2020unsupervised.pdf)] |

#### Multimodal Language Models

| _Year_ | <div style="width:120px">_Authors_</div> | <div style="width:120px">_Conf._</div> | _Title_ | _Links_ |
| :----: | ---------------------------------------- | -------------------------------------- | ------- | ------- |
| 2011.11| Jia et al. | ICCV'11 | Learning Cross-modality Similarity for Multinomial Data | [[pdf](https://people.eecs.berkeley.edu/~trevor/iccv11-mm.pdf)] [[repo](paper/jia2011learning.pdf)] | 
| 2014.10| Mao et al. | arXiv | Explain Images with Multimodal Recurrent Neural Networks | [[pdf](https://arxiv.org/pdf/1410.1090.pdf)] [[repo](paper/mao2014explain.pdf)] |
| 2014.11| Kiros et al. | arXiv | Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models | [[pdf](https://arxiv.org/pdf/1411.2539.pdf)] [[repo](paper/kiros2014unifying.pdf)] | 
| 2015.06| Mao et al. | ICLR'15 | Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN) | [[pdf](https://arxiv.org/pdf/1412.6632.pdf)] [[repo](paper/mao2014deep.pdf)] [[github](https://github.com/mjhucla/mRNN-CR)] |
| 2015.09| Ferraro et al. | EMNLP'15 | A Survey of Current Datasets for Vision and Language Research | [[pdf](https://aclweb.org/anthology/D15-1021)] [[repo](paper/ferraro2015survey.pdf)] |
| 2015.12| Ma et al. | ICCV'15 | Multimodal Convolutional Neural Networks for Matching Image and Sentence | [[pdf](http://openaccess.thecvf.com/content_iccv_2015/papers/Ma_Multimodal_Convolutional_Neural_ICCV_2015_paper.pdf)] [[repo](paper/ma2015multimodal.pdf)] |
| 2016.06| You et al. | CVPR'16 | Image Captioning with Semantic Attention | [[pdf](http://openaccess.thecvf.com/content_cvpr_2016/papers/You_Image_Captioning_With_CVPR_2016_paper.pdf)] [[repo](paper/you2016image.pdf)] |
| 2016.10| Lu et al. | NIPS'16 | Hierarchical Question-Image Co-Attention for Visual Question Answering | [[pdf](https://arxiv.org/pdf/1606.00061.pdf)] [[repo](paper/lu2016hierarchical.pdf)] [[github](https://github.com/jiasenlu/HieCoAttenVQA)] |
| 2016.10| Yang et al. | NIPS'16 | Review Networks for Caption Generation | [[pdf](https://arxiv.org/pdf/1605.07912.pdf)] [[repo](paper/yang2016review.pdf)] [[github](https://github.com/kimiyoung/review_net)] |
| 2018.06| Wang et al. | NAACL'18 | Object Counts! Bringing Explicit Detections Back into Image Captioning | [[pdf](https://www.aclweb.org/anthology/N18-1198/)] [[repo](paper/wang2018object.pdf)] |
| 2018.06| Anderson et al. | CVPR'18 | Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering | [[pdf](http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf)] [[repo](paper/anderson2018bottom.pdf)] |
| 2018.06| Nguyen et al. | CVPR'18 | Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering | [[pdf](https://arxiv.org/pdf/1804.00775.pdf)] [[repo](paper/nguyen2018improved.pdf)] |
| 2018.10| Zhu et al. | EMNLP'18 | MSMO: Multimodal Summarization with Multimodal Output | [[pdf](https://www.aclweb.org/anthology/D18-1448.pdf)] [[repo](paper/zhu2018MSMO.pdf)] |
| 2019.02| Li et al. | AAAI'19 | Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering | [[pdf](https://pdfs.semanticscholar.org/5653/59aac8914505e6b02db05822ee63d3ffd03a.pdf?_ga=2.221376792.1635135941.1571362744-1866174129.1565321067)] [[repo](paper/li2019beyond.pdf)] |
| 2019.06| Qin et al. | CVPR'19 | Look Back and Predict Forward in Image Captioning | [[pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_Look_Back_and_Predict_Forward_in_Image_Captioning_CVPR_2019_paper.pdf)] [[repo](paper/qin2019look.pdf)] |
| 2019.06| Yu et al. | CVPR'19 | Deep Modular Co-Attention Networks for Visual Question Answering | [[pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yu_Deep_Modular_Co-Attention_Networks_for_Visual_Question_Answering_CVPR_2019_paper.pdf)] [[repo](paper/yu2019deep.pdf)] |
| 2019.09| Kiela et al. | arXiv | Supervised Multimodal Bitransformers for Classifying Images and Text | [[pdf](https://vigilworkshop.github.io/static/papers/40.pdf)] [[repo](paper/kiela2019supervised.pdf)] [[github](https://github.com/facebookresearch/mmbt/)] |
| 2019.10| Guo et al. | ACM-MM'19 | Aligning Linguistic Words and Visual Semantic Units for Image Captioning | [[pdf](https://arxiv.org/pdf/1908.02127.pdf)] [[repo](paper/guo2019aligning.pdf)] [[github](https://github.com/ltguo19/VSUA-Captioning)] |
| 2019.10| Li et al. | ACM-MM'19 | Walking with MIND: Mental Imagery eNhanceD Embodied QA | [[pdf](https://arxiv.org/pdf/1908.01482.pdf)] [[repo](paper/li2019walking.pdf)] |
| 2019.10| Wu et al. | ACM-MM'19 | Editing Text in the Wild | [[pdf](https://arxiv.org/pdf/1908.03047.pdf)] [[repo](paper/wu2019editing.pdf)] |
| 2019.12| Khademi et al. | NIPS'19 | Multimodal Neural Graph Memory Networks for Visual Question Answering | [[pdf](https://grlearning.github.io/papers/32.pdf)] [[repo](paper/khademi2019multimodal.pdf)] |
| 2020.01| Park et al. | WACV'20 | MHSAN: Multi-Head Self-Attention Network for Visual Semantic Embedding | [[pdf](https://arxiv.org/pdf/2001.03712.pdf)] [[repo](paper/park2020mhsan.pdf)] |
| 2020.02| Mai et al. | AAAI'20 | Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion | [[pdf](https://arxiv.org/pdf/1911.07848.pdf)] [[repo](paper/mai2020modality.pdf)] |
| 2020.02| Sun et al. | AAAI'20 | Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis | [[pdf](https://arxiv.org/pdf/1911.05544.pdf)] [[repo](paper/sun2020learning.pdf)] |
| 2020.02| Zhang et al. | AAAI'20 | Learning Long- and Short-Term User Literal Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption | [[pdf](https://weizhangltt.github.io/paper/zhang-aaai20.pdf)] [[repo](paper/zhang2020learning.pdf)] |
| 2020.07| Alikhani et al. | ACL'20 | Clue: Cross-modal Coherence Modeling for Caption Generation | [[pdf](https://arxiv.org/pdf/2005.00908.pdf)] [[repo](paper/alikhani2020clue.pdf)] |
| 2020.07| Chauhan et al. | ACL'20 | Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sacrasm, Sentiment and Emotion Analysis | [[pdf](https://www.iitp.ac.in/~ai-nlp-ml/papers/acl20-multiSA.pdf)] [[repo](paper/chauhan2020sentiment.pdf)] |
| 2020.07| Lin et al. | ACL'20 | A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks | [[pdf](https://arxiv.org/pdf/2005.09606.pdf)] [[repo](paper/lin2020recipe.pdf)] [[github](https://github.com/microsoft/multimodal-aligned-recipe-corpus)] |

#### Neural Machine Translation

| _Year_ | <div style="width:120px">_Authors_</div> | <div style="width:120px">_Conf._</div> | _Title_ | _Links_ |
| :----: | ---------------------------------------- | -------------------------------------- | ------- | ------- |
| 2016.06| Yang et al. | NAACL-HLT'16 | Hierarchical Attention Networks for Document Classification | [[pdf](https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf)] [[repo](paper/yang2016hierarchical.pdf)] |
| 2016.06| Zoph et al. | arXiv | Multi-Source Neural Translation | [[pdf](https://pdfs.semanticscholar.org/e4c3/aa6fef525c9ed4688125ef932a2afbbae851.pdf)] [[repo](paper/zoph2016multi.pdf)] |
| 2017.12| Vaswani et al. | NIPS'17 | Attention Is All You Need | [[pdf](https://arxiv.org/pdf/1706.03762.pdf)] [[repo](paper/vaswani2017attention.pdf)] [[github](https://github.com/tensorflow/tensor2tensor)] |
| 2017.12| Xia et al. | NIPS'17 | Deliberation Networks: Sequence Generation Beyond One-Pass Decoding | [[pdf](https://papers.nips.cc/paper/6775-deliberation-networks-sequence-generation-beyond-one-pass-decoding.pdf)] [[repo](paper/xia2017delibertaion.pdf)] [[github](https://github.com/ustctf/delibnet)] |
| 2018.04| Yang et al. | NAACL-HLT'18 | Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets | [[pdf](https://arxiv.org/pdf/1703.04887.pdf)] [[repo](paper/yang2018improving.pdf)] |
| 2018.09| Wu et al. | NAACL-HLT'18 | Adversarial Neural Machine Translation | [[pdf](https://arxiv.org/pdf/1704.06933.pdf)] [[repo](paper/wu2018adversarial.pdf)] |
| 2018.10| Miculicich et al. | EMNLP'18 | Document-Level Neural Machine Translation with Hierarchical Attention Networks | [[pdf](https://www.aclweb.org/anthology/D18-1325)] [[repo](paper/miculicich2018document.pdf)] |
| 2019.05| Devlin et al. | arXiv | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | [[pdf](https://arxiv.org/pdf/1810.04805.pdf)] [[repo](paper/devlin2019bert.pdf)] [[github](https://github.com/google-research/bert)] |
| 2019.05| Zhou et al. | arXiv | Synchronous Bidirectional Neural Machine Translation | [[pdf](https://arxiv.org/pdf/1905.04847.pdf)] [[repo](paper/zhou2019synchronous.pdf)] [[github](https://github.com/wszlong/sb-nmt)] |
| 2019.06| Yang et al. | arXiv | XLNet: Generalized Autoregressive Pretraining for Language Understanding | [[pdf](https://arxiv.org/pdf/1906.08237.pdf)] [[repo](paper/yang2019xlnet.pdf)] [[github](https://github.com/zihangdai/xlnet)] |
| 2019.07| Dai et al. | ACL'19 | Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context | [[pdf](https://arxiv.org/pdf/1901.02860.pdf)] [[repo](paper/dai2019transformerxl.pdf)] [[github](https://github.com/kimiyoung/transformer-xl)] |
| 2019.07| Liu et al. | ACL'19 | Hierarchical Transformers for Multi-Document Summarization | [[pdf](https://www.aclweb.org/anthology/P19-1500)] [[repo](paper/liu2019hierarchical.pdf)] [[github](https://github.com/nlpyang/hiersumm)] | 
| 2019.07| Pourdamghani et al. | ACL'19 | Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation | [[pdf](https://arxiv.org/pdf/1906.05683.pdf)] [[repo](paper/pourdamghani2019translating.pdf)] |

## Datasets

| _Dataset_ | <div style="width:120px">_Authors_</div> | _Paper_ | _Links_ |
| --------- | ---------------------------------------- | ------- | ------- |
| Flickr30K | Young et al. | From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions | [[pdf](https://www.aclweb.org/anthology/Q14-1006)] [[repo](datasets/Flickr30K.pdf)] [[web](http://shannon.cs.illinois.edu/DenotationGraph/)] |
| Flickr30K Entities | Plummer et al. | Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models | [[pdf](https://arxiv.org/pdf/1505.04870.pdf)] [[repo](datasets/Flickr30K_entities.pdf)] [[web](http://bryanplummer.com/Flickr30kEntities/)] [[github](https://github.com/BryanPlummer/flickr30k_entities)] | 
| Multi30K | Elliott et al. | Multi30K: Multilingual English-German Image Descriptions | [[pdf](https://arxiv.org/pdf/1605.00459.pdf)] [[repo](datasets/Multi30K.pdf)] [[github](https://github.com/multi30k/dataset)] |
| IAPR-TC12 | Grubinger et al. | The IAPR TC-12 Benchmark: A New Evaluation Resource for Visual Information Systems | [[pdf](http://thomas.deselaers.de/publications/papers/grubinger_lrec06.pdf)] [[repo](datasets/IAPR-TC12.pdf)] [[web](https://www.imageclef.org/photodata)] |
| VATEX | Wang et al. | VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research | [[pdf](https://arxiv.org/pdf/1904.03493.pdf)] [[repo](datasets/VATEX.pdf)] [[web](http://vatex.org/main/download.html)] |

## Metrics

| _Metric_ | <div style="width:120px">_Authors_</div> | _Paper_ | _Links_ |
| -------- | ---------------------------------------- | ------- | ------- |
| BLEU | Papineni et al. | BLEU: a Method for Automatic Evaluation of Machine Translation | [[pdf](https://www.aclweb.org/anthology/P02-1040)] [[repo](metrics/papineni2002bleu.pdf)] |
| METEOR | Banerjee et al. | METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments | [[pdf](http://www.cs.cmu.edu/~alavie/METEOR/pdf/Banerjee-Lavie-2005-METEOR.pdf)] [[repo](metrics/banerjee2005meteor.pdf)] [[web](http://www.cs.cmu.edu/~alavie/METEOR/)] |
| METEOR 1.5 | Denkowski et al. | METEOR Universal: Language Specific Translation Evaluation for Any Target Language | [[pdf](https://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf)] [[repo](metrics/denkowski2014meteor-1.5.pdf)] [[web](http://www.cs.cmu.edu/~alavie/METEOR/)] | 
| TER | Snover et al. | A study of Translation Edit Rate with Targeted Human Annotation | [[pdf](https://www.cs.umd.edu/~snover/pub/amta06/ter_amta.pdf)] [[repo](metrics/snover2006ter.pdf)] |

## Tutorials

| _Year_ | _Authors_ | _Title_ | _Links_ |
| :----: | --------- | ------- | ------- |
| 2016 | Elliott et al. | Multimodal Learning and Reasoning | [[pdf](https://github.com/MultimodalNLP/MultimodalNLP.github.io/raw/master/mlr_tutorial.pdf)] [[repo](tutorials/elliott2016multimodal.pdf)] |
| 2017 | Lucia Specia | Multimodal Machine Translation | [[pdf](https://mtm2017.unbabel.com/assets/images/slides/lucia_specia.pdf)] [[repo](tutorials/specia2017multimodal.pdf)] |
| 2018 | Loic Barrault | Introduction to Multimodal Machine Translation | [[pdf](https://www.clsp.jhu.edu/wp-content/uploads/sites/75/2018/06/2018-06-22-Barrault-Multimodal-MT.pdf)] [[repo](tutorials/barrault2018introduction.pdf)] |
| 2018 | Mirella Lapata | Understanding Visual Scenes | [[pdf](http://tcci.ccf.org.cn/conference/2017/dldoc/invtalk01_mL.pdf)] [[repo](tutorials/lapata2018understanding.pdf)] |
