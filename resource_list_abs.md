**Resource List w/ abstract**

- [Paper](#paper)
- [Datasets](#datasets)
- [Metrics](#metrics)
- [Tutorials](#tutorials)

## Paper

**2011** {#1}

#### Learning Cross-modality Similarity for Multinomial Data [[pdf](https://people.eecs.berkeley.edu/~trevor/iccv11-mm.pdf)] <!-- omit in toc -->
  * Jia et al., (2011)
  * ICCV'11
  * Many applications involve multiple-modalities such as text and images that describe the problem of interest. In order to leverage the information present in all the modalities, one must model the relationships between them. While some techniques have been proposed to tackle this problem, they either are restricted to words describing visual objects only, or require full correspondences between the different modalities. As a consequence, they are unable to tackle more realistic scenarios where a narrative text is only loosely related to an image, and where only a few image-text pairs are available. In this paper, we propose a model that addresses both these challenges. Our model can be seen as a Markov random field of topic models, which connects the documents based on their similarity. As a consequence, the topics learned with our model are shared across connected documents, thus encoding the relations between different modalities. We demonstrate the effectiveness of our model for image retrieval from a loosely related text.

**2014** {#2}

#### Explain Images with Multimodal Recurrent Neural Networks [[pdf](https://arxiv.org/pdf/1410.1090.pdf)] <!-- omit in toc -->
  * Mao et al., (2014)
  * arXiv
  * In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative model. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.

#### Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models [[pdf](https://arxiv.org/pdf/1411.2539.pdf)] <!-- omit in toc -->
  * Kiros et al., (2014)
  * arXiv
  * Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. \* image of a blue car \* - "blue" + "red" is near images of red cars. Sample captions generated for 800 images are made available for comparison.

**2016** {#3}

#### Does Multimodality Help Human and Machine for Translation and Image Captioning? [[pdf](https://arxiv.org/pdf/1605.09186.pdf)] <!-- omit in toc -->
  * Caglayan et al., (2016)
  * WMT'16
  * This paper presents the systems developed by LIUM and CVC for the WMT16 Multimoadl Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR

#### Multimodal Attention for Neural Machine Translation [[pdf](https://arxiv.org/pdf/1609.03976.pdf)] <!-- omit in toc -->
  * Caglayan et al., (2016) 
  * arXiv
  * The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30K multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.

#### Attention-based Multimodal Neural Machine Translation [[pdf](https://www.aclweb.org/anthology/W16-2360)] <!-- omit in toc -->
  * Huang et al., (2016)
  * WMT'16
  * We present a novel neural machine translation (NMT) architecture associating visual and textual features for translation tasks with multiple modalities. Transformed global and regional visual features are concatenated with text to form attend-able sequences which are dissipated over parallel long short-term memory (LSTM) threads to assist the encoder generating a representation for attention-based decoding. Experiments show that the proposed NMT outperform the text-only baseline.

**2017** {#6}

#### A Teacher-Student Framework for Zero-Resource Neural Machine Translation [[pdf](https://arxiv.org/pdf/1705.00753.pdf)] <!-- omit in toc -->
  * Chen et al., (2017)
  * arXiv
  * While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low-resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming the parallel sentences have close probabilities of generating a sentence in a third language. Based on this assumption, our method is able to train a source-to-target NMT model ("student") without parallel corpora available, guided by an existing pivot-to-target NMT model ("teacher") on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.

#### Zero-resource Machine Translation by Multimodal Encoder-decoder Network with Multimedia Pivot [[pdf](https://arxiv.org/pdf/1611.04503.pdf)] <!-- omit in toc -->
  * Nakayama et al., (2017)
  * arXiv
  * We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (i.e., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the *pivot*, we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In the experiment, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.

#### Multimodal Compact Bilinear Pooling for Multimodal Neural Machine Translation [[pdf](https://arxiv.org/pdf/1703.08084.pdf)] <!-- omit in toc -->
  * Delbrouck et al., (2017)
  * ICLR'17
  * In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus on both sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes teh outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.

#### Unraveling the Contribution of Image Captioning and Neural Machine Translation for Multimodal Machine Translation [[pdf](https://ufal.mff.cuni.cz/pbml/108/art-lala-madhyastha-wang-specia.pdf)] <!-- omit in toc -->
  * Lala et al., (2017)
  * PBML'17
  * Recent work on multimodal machine translation has attempted to address the problem of producing target language image descriptions based on both the source language description and the corresponding image. However, existing work has not been conclusive on the contribution of visual information. This paper presents an in-depth study of the problem by examining the differences and complementarities of two related but distinct approaches to this task: text-only neural machine translation and image captioning. We analyse the scope for improvement and the effect of different data and settings to build models for these tasks. We also propose ways of combining these two approaches for improved translation quality.

#### Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description [[pdf](https://arxiv.org/abs/1710.07177)] <!-- omit in toc -->
  * Elliott et al., (2017)
  * WMT'17
  * We present the results from the second shared task on multimodal machine translation and multilingual image description. Nine teams submitted 19 systems to two tasks. The multimodal translation task, in which the source sentence is supplemented by an image, was extended with a new language (French) and two new test sets. The multilingual image description task was changed such that at test time, only the image is given. Compared to last year, multimodal systems improved, but text-only systems remain competitive.

#### Attention Is All You Need [[pdf](https://arxiv.org/pdf/1706.03762.pdf)] [[github](https://github.com/tensorflow/tensor2tensor)] <!-- omit in toc -->
  * Vaswani et al., (2017)
  * NIPS'17
  * The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

**2018** {#11}

#### Findings of the Third Shared Task on Multimodal Machine Translation [[pdf](http://statmt.org/wmt18/pdf/WMT029.pdf)] <!-- omit in toc -->
  * Barrault et al., (2018)
  * WMT'18
  * We present the results from the third shared task on multimodal machine translation. In this task a source sentence in English is supplemented by an image and participating systems are required to generate a translation for such a sentence into German, French or Czech. The image can be used in addition to (or instead of) the source sentence. This year the task was extended with a third target language (Czech) and a new test set. In addition, a variant of this task was introduced with its own test set where the source sentence is given in multiple languages: English, French and German, and participating systems are required to generate a translation in Czech. Seven teams submitted 45 different systems to the two variants of the task. Compared to last year, the performance of the multimodal submissions improved, but text-only systems remain competitive.

#### LIUM-CVC Submissions for WMT18 Multimodal Translation Task [[pdf](http://statmt.org/wmt18/pdf/WMT065.pdf)] <!-- omit in toc -->
  * Caglayan et al., (2018)
  * WMT'18
  * This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using encoder-side information. Our final constrained submissions ranked first for English -> French and second for English -> German language pairs among the constrained submissions to the automatic evaluation metric METEOR.

#### The MeMAD Submission to the WMT18 Multimodal Translation Task [[pdf](http://statmt.org/wmt18/pdf/WMT066.pdf)] <!-- omit in toc -->
  * Gronroos et al., (2018)
  * WMT'18
  * This paper describes the MeMAD project entry to the WMT Multimodal Machine Translation Shared Task. We propose adapting the Transformer neural machine translation (NMT) architecture to a multi-modal setting. In this paper, we also describe the preliminary experiments with text-only translation systems leading us up to this choice. We have the top scoring system for both English-to-German and English-to-French, according to the automatic metrics for flickr18. Our experiments show that the effect of the visual features in our system is small. Our largest gains come from the quality of the underlying text-only NMT system. We find that appropriate use of additional data is effective.

#### The AFRL-Ohio State WMT18 Multimodal System: Combining Visual with Traditional [[pdf](http://statmt.org/wmt18/pdf/WMT067.pdf)] <!-- omit in toc -->
  * Gwinnup et al., (2018)
  * WMT'18
  * AFRL-Ohio State extends its useage of visual domain-driven machine translation for use as a peer with traditional machine translation systems. As a peer, it is enveloped into a system combination of neural and statistical MT systems to present a composite translation.

#### CUNI System for the WMT18 Multimodal Translation Task [[pdf](http://statmt.org/wmt18/pdf/WMT068.pdf)] <!-- omit in toc -->
  * Helcl et al., (2018)
  * WMT'18
  * We present our submission to the WMT18 Multimodal Translation Task. The main feature of our submission is applying a self-attentive network instead of a recurrent neural network. We evaluate two methods of incorporating the visual features in the model: first, we include the image representation as another input to the network; second, we train the model to predict the visual features and use it as an auxiliary objective. For our submission, we acquired both textual and multimodal additional data. Both of the proposed methods yield significant improvements over recurrent networks and self-attentive textual baselines.

#### Sheffield Submissions for WMT18 Multimodal Translation Shared Task [[pdf](http://statmt.org/wmt18/pdf/WMT069.pdf)] <!-- omit in toc -->
  * Lala et al., (2018)
  * WMT'18
  * This paper describes the University of Sheffield's submissions to the WMT18 Multimodal Machine Translation shared task. We participated in both task 1 and 1b. For task 1, we build on a standard sequence to sequence attention-based neural machine translation system (NMT) and investigate the utility of multimodal re-ranking approaches. More specifically, n-best translation candidates from this system are re-ranked using novel multimodal cross-lingual word sense disambiguation models. For task 1b, we explore three approaches: (i) re-ranking based on cross-lingual word sense disambiguation (as for task 1), (ii) re-ranking based on consensus of NMT n-best lists from German-Czech, French-Czech and English-Czech systems, and (iii) data augmentation by generating English source data through machine translation from French to English and from German to English followed by hypothesis selection using a multimodal-reranker.

#### Ensemble Sequence Level Training for Multimodal MT: OSU-Baidu WMT18 Multimodal Translation System Report [[pdf](http://statmt.org/wmt18/pdf/WMT070.pdf)] <!-- omit in toc -->
  * Zheng et al., (2018)
  * WMT'18
  * This paper describes multimodal machine translation systems developed jointly by Oregon State University and Baidu Research for WMT 2018 Shared Task on multimodal translation. In this paper, we introduce a simple approach to incorporate image information by feeding image features to the decoder side. We also explore different sequence level training methods including scheduled sampling and reinforcement learning which lead to substantial improvements. Our systems ensemble several models using different architectures and training methods and achieve the best performance for three subtasks: En-De and En-Cs in task 1 and (En+De+Fr)-Cs task 1B.

#### UMONS Submission for WMT18 Multimodal Translation Task [[pdf](http://statmt.org/wmt18/pdf/WMT071.pdf)] <!-- omit in toc -->
  * Delbrouck et al., (2018)
  * WMT'18
  * This paper describes the UMONS solution for the Multimodal Machine Translation Task presented at the third conference on machine translation (WMT18). We explore a novel architecture, called deepGRU, based on recent findings in the related task of Neural Image Captioning (NIC). The models presented in the following sections lead to the best METEOR translation score for both constrained (English, image) -> German and (English, image) -> French sub-tasks.

#### A Visual Attention Grounding Neural Model for Multimodal Machine Translation [[pdf](https://www.aclweb.org/anthology/D18-1400)] <!-- omit in toc -->
  * Zhou et al., (2018)
  * ACL'18
  * We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our model jointly optimizes the learning of a shared visual-language embedding and a translator. The model leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this dataset, our visual attention grounding model outperforms other methods by a large margin. 

#### Document-Level Neural Machine Translation with Hierarchical Attention Networks [[pdf](https://www.aclweb.org/anthology/D18-1325)] <!-- omit in toc -->
  * Miculicich et al., (2018)
  * EMNLP'18
  * Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model's own previous hidden state. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.

#### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[pdf](https://arxiv.org/pdf/1810.04805.pdf)] [[github](https://github.com/google-research/bert)] <!-- omit in toc -->
  * Devlin et al., (2019)
  * arXiv
  * We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

**2019** {#7}

#### Probing the Need for Visual Context in Multimodal Machine Translation [[pdf](https://arxiv.org/pdf/1903.08678.pdf)] <!-- omit in toc -->
  * Caglayan et al., (2019)
  * NAACL-HLT'19
  * Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.

#### Unsupervised Multi-modal Neural Machine Translation [[pdf](https://arxiv.org/pdf/1811.11365.pdf)] <!-- omit in toc -->
  * Su et al., (2019)
  * CVPR'19
  * Unsupervised neural machine translation (UNMT) has recently achieved remarkable results with only large monolingual corpora in each language. However, the uncertainty of associating target with source sentences makes UNMT theoretically an ill-posed problem. This work investigates the possibility of utilizing images for disambiguation to improve the performance of UNMT. Our assumption is intuitively based on the invariant property of image, i.e., the description of the same visual content by different languages should be approximately similar. We propose an unsupervised multi-modal machine translation (UMNMT) framework based on the language translation cycle consistency loss conditional on the image, targeting to learn the bidirectional multi-modal and uni-modal, our inference model can translate with or without the image. On the widely used Multi30K dataset, the experimental results of our approach are significantly better than those of the text-only UNMT on the 2016 test dataset.

#### Distilling Translations with Visual Awareness [[pdf](https://arxiv.org/pdf/1906.07701.pdf)] <!-- omit in toc -->
  * Ive et al., (2019)
  * ACL'19
  * Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage detector. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.

#### Latent Variable Model for Multi-modal Translation [[pdf](https://www.aclweb.org/anthology/P19-1642)] <!-- omit in toc -->
  * Calixto et al., (2019)
  * ACL'19
  * In this work, we propose to model the interaction between visual and textual features for multi-modal neural machine translation (MMT) through a latent variable model. This latent variable can be seen as a multi-modal stochastic embedding of an image and its description in a foreign language. It is used in a target-language decoder and also to predict image features. Importantly, our model formulation utilizes visual and textual inputs during training but does not require those images be available at test time. We show that our latent variable MMT formulation improves considerably over strong baselines, including a multi-task learning approach and a conditional variational auto-encoder approach. Finally, we show improvements due to (i) predicting image features in addition to only conditioning on them, (ii) imposing a constraint on the KL term to promote models with non-negligible mutual information between inputs and latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data).

#### From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots [[pdf](https://arxiv.org/pdf/1906.00872.pdf)] <!-- omit in toc -->
  * Chen et al., (2019)
  * arXiv
  * The neural machine translation model has suffered from the lack of large-scale parallel corpora. In contrast, we humans can learn multi-lingual translations even without parallel texts by referring our languages to the external world. To mimic such human learning behaviour, we employ images as pivots to enable zero-resource translation learning. However, a picture tells a thousand words, which makes multi-lingual sentences pivoted by the same image noisy as mutual translations and thus hinders the translation model learning. In this work, we propose a progressive learning approach for image pivoted zero-resource machine translation. Since words are less diverse when grounded in the image, we first learn word-level translation with image pivots, and then progress to learn the sentence-level translation by utilizing the learned word translation to suppress noises in image-pivoted multi-lingual sentences. Experimental results on two widely used image-pivot translation datasets, IAPR-TC12 and Multi30K, show that the proposed approach significantly outperforms other state-of-the-art methods.

#### Look Back and Predict Forward in Image Captioning [[pdf](http://openaccess.thecvf.com/content_CVPR_2019/papers/Qin_Look_Back_and_Predict_Forward_in_Image_Captioning_CVPR_2019_paper.pdf)] <!-- omit in toc -->
  * Qin et al., (2019)
  * CVPR'19
  * Most existing attention-based methods on image captioning focus on the current word and visual information in one time step and generate the next word, without considering the visual and linguistic coherence. We propose Look Back (LB) method to embed visual information from the past and Predict Forward (PF) approach to look into future. LB method introduces attention value from the previous time step into the current attention generation to suit visual coherence of human. PF model predicts the next two words in one step and jointly employs their probabilities for inference. Then the two approaches are combined together as LBPF to further integrate visual information from the past and linguistic information in the future to improve image captioning performance. All the three methods are applied on a classic base decoder, and show remarkable improvements on MSCOCO dataset with small increments on parameter counts. Our LBPF model achieves BLEU-4 / CIDEr / SPICE scores of 37.4 / 116.4 / 21.2 with cross-entropy loss and 38.3 / 127.6 / 22.0 with CIDEr optimization. Our three proposed methods can be easily applied on most attention-based encoder-decoder models for image captioning.

#### Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[pdf](https://arxiv.org/pdf/1901.02860.pdf)] [[github](https://github.com/kimiyoung/transformer-xl)] <!-- omit in toc -->
  * Dai et al., (2019)
  * ACL'19
  * Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modelling. We propose a novel neural architecture *Transformer-XL* that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on en-wiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.

## Datasets

#### Flickr30K [[pdf](https://www.aclweb.org/anthology/Q14-1006)] [[web](http://shannon.cs.illinois.edu/DenotationGraph/)] <!-- omit in toc -->
  * From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
  * Young et al., (2014)
  * We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.

#### Flickr30K Entities [[pdf](https://arxiv.org/pdf/1505.04870.pdf)] [[web](http://bryanplummer.com/Flickr30kEntities/)] [[github](https://github.com/BryanPlummer/flickr30k_entities)] <!-- omit in toc -->
  * Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
  * Plummer et al., (2016)
  * The Flickr30K dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30K Entities, which augments the 158k captions from Flickr30K with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.

#### Multi30K [[pdf](https://arxiv.org/pdf/1605.00459.pdf)] [[github](https://github.com/multi30k/dataset)] <!-- omit in toc -->
  * Multi30K: Multilingual English-German Image Descriptions
  * Elliott et al., (2016)
  * We introduce the **Multi30K** dataset to simulate multilingual multimodal research. Recent advances in image description have been demonstrated on English-language datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flicker30K dataset with i) German translations created by professional translators over a subset of the English descriptions. We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks.

#### IAPR-TC12 [[pdf](http://thomas.deselaers.de/publications/papers/grubinger_lrec06.pdf)] [[web](https://www.imageclef.org/photodata)] <!-- omit in toc -->
  * The IAPR TC-12 Benchmark: A New Evaluation Resource for Visual Information Systems
  * Grubinger et al., (2006)
  * In this paper, we describe an image collection created for the CLEF cross-language image retrieval task (ImageCLEF). This image retrieval benchmark (referred to as the IAPR TC-12 Benchmark) has developed from an initiative started by the Technical Committee 12 (TC-12) of the International Association of Pattern Recognition (IAPR). The collection consists of 20,000 images from a private photographic image collection. The construction and composition of the IAPR TC-12 Benchmark is described, including its associated text captions which are expressed in multiple languages, making the collection well-suited for evaluating the effectiveness of both text-based and visual retrieval methods. We also discuss the current and expected uses of the collection, including its use to benchmark and compare different image retrieval systems in ImageCLEF 2006.

## Metrics

#### BLEU [[pdf](https://www.aclweb.org/anthology/P02-1040)] <!-- omit in toc -->
  * BLEU: a Method for Automatic Evaluation of Machine Translation
  * Papineni et al., (2002)
  * Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that cannot be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.

#### METEOR [[pdf](http://www.cs.cmu.edu/~alavie/METEOR/pdf/Banerjee-Lavie-2005-METEOR.pdf)] [[web](http://www.cs.cmu.edu/~alavie/METEOR/)] <!-- omit in toc -->
  * METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments
  * Banerjee et al., (2005)
  * We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine-produced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessment of teh LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-by-segment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigram-precision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules.

#### METEOR 1.5 [[pdf](https://www.cs.cmu.edu/~alavie/METEOR/pdf/meteor-1.5.pdf)] [[web](http://www.cs.cmu.edu/~alavie/METEOR/)] <!-- omit in toc -->
  * METEOR Universal: Language Specific Translation Evaluation for Any Target Language
  * Denkowski et al., (2014)
  * This paper describes METEOR Universal, released for the 2014 ACL Workshop on Statistical Machine Translation. METEOR Universal brings language specific evaluation to previously unsupported target language by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. METEOR Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14).

#### TER [[pdf](https://www.cs.umd.edu/~snover/pub/amta06/ter_amta.pdf)] <!-- omit in toc -->
  * A study of Translation Edit Rate with Targeted Human Annotation
  * Snover et al., (2006)
  * We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU - even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and the four-reference variants of TER, and HTER correlate with human judgments as well as - or better than - a second human judgment does.

## Tutorials

#### Multimodal Learning and Reasoning [[pdf](https://github.com/MultimodalNLP/MultimodalNLP.github.io/raw/master/mlr_tutorial.pdf)] <!-- omit in toc -->
  * Elliott et al., University of Amsterdam (2016)

#### Multimodal Machine Translation [[pdf](https://mtm2017.unbabel.com/assets/images/slides/lucia_specia.pdf)] <!-- omit in toc -->
  * Lucia Specia, University of Sheffield (2017)

#### Introduction to Multimodal Machine Translation [[pdf](https://www.clsp.jhu.edu/wp-content/uploads/sites/75/2018/06/2018-06-22-Barrault-Multimodal-MT.pdf)] <!-- omit in toc -->
  * Loic Barrault, University of Le Mans (2018)

