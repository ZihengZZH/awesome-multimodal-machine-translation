# Resource List w/ abstract

* Papers
* Datasets
* Tutorials

## Papers

**2014**

* Explain Images with Multimodal Recurrent Neural Networks [[pdf](https://arxiv.org/pdf/1410.1090.pdf)]
  * Mao et al., (2014)
  * arXiv
  * In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel sentence descriptions to explain the content of images. It directly models the probability distribution of generating a word given previous words and the image. Image descriptions are generated by sampling from this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on three benchmark datasets (IAPR TC-12, Flickr 8K, and Flickr 30K). Our model outperforms the state-of-the-art generative model. In addition, the m-RNN model can be applied to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval.

**2015**

* 

**2016**

* Does Multimodality Help Human and Machine for Translation and Image Captioning? [[pdf](https://arxiv.org/pdf/1605.09186.pdf)]
  * Caglayan et al., (2016)
  * WMT'16
  * This paper presents the systems developed by LIUM and CVC for the WMT16 Multimoadl Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR

**2018**

* LIUM-CVC Submissions for WMT18 Multimodal Translation Task
  * Caglayan et al., (2018)
  * WMT'18
  * This paper describes the multimodal Neural Machine Translation systems developed by LIUM and CVC for WMT18 Shared Task on Multimodal Translation. This year we propose several modifications to our previous multimodal attention architecture in order to better integrate convolutional features and refine them using encoder-side information. Our final constrained submissions ranked first for English -> French and second for English -> German language pairs among the constrained submissions to the automatic evaluation metric METEOR.

* The MeMAD Submission to the WMT18 Multimodal Translation Task
  * Gronroos et al., (2018)
  * WMT'18
  * This paper describes the MeMAD project entry to the WMT Multimodal Machine Translation Shared Task. We propose adapting the Transformer neural machine translation (NMT) architecture to a multi-modal setting. In this paper, we also describe the preliminary experiments with text-only translation systems leading us up to this choice. We have the top scoring system for both English-to-German and English-to-French, according to the automatic metrics for flickr18. Our experiments show that the effect of the visual features in our system is small. Our largest gains come from the quality of the underlying text-only NMT system. We find that appropriate use of additional data is effective.

* The AFRL-Ohio State WMT18 Multimodal System: Combining Visual with Traditional
  * Gwinnup et al., (2018)
  * WMT'18
  * AFRL-Ohio State extends its useage of visual domain-driven machine translation for use as a peer with traditional machine translation systems. As a peer, it is enveloped into a system combination of neural and statistical MT systems to present a composite translation.

* CUNI System for the WMT18 Multimodal Translation Task
  * Helcl et al., (2018)
  * WMT'18

* Sheffield Submissions for WMT18 Multimodal Translation Shared Task
  * Lala et al., (2018)
  * WMT'18

* Ensemble Sequence Level Training for Multimodal MT: OSU-Baidu WMT18 Multimodal Translation System Report
  * Zheng et al., (2018)
  * WMT'18

* UMONS Submission for WMT18 Multimodal Translation Task
  * Delbrouck et al., (2018)
  * WMT'18

**2019**

* Probing the Need for Visual Context in Multimodal Machine Translation [[pdf](https://arxiv.org/pdf/1903.08678.pdf)]
  * Caglayan et al., (2019)
  * NAACL-HLT'19
  * Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.

* Unsupervised Multi-modal Neural Machine Translation [[pdf](https://arxiv.org/pdf/1811.11365.pdf)]
  * Su et al., (2019)
  * CVPR'19
  * Unsupervised neural machine translation (UNMT) has recently achieved remarkable results with only large monolingual corpora in each language. However, the uncertainty of associating target with source sentences makes UNMT theoretically an ill-posed problem. This work investigates the possibility of utilizing images for disambiguation to improve the performance of UNMT. Our assumption is intuitively based on the invariant property of image, i.e., the description of the same visual content by different languages should be approximately similar. We propose an unsupervised multi-modal machine translation (UMNMT) framework based on the language translation cycle consistency loss conditional on the image, targeting to learn the bidirectional multi-modal and uni-modal, our inference model can translate with or without the image. On the widely used Multi30K dataset, the experimental results of our approach are significantly better than those of the text-only UNMT on the 2016 test dataset.

* Distilling Translations with Visual Awareness [[pdf](https://arxiv.org/pdf/1906.07701.pdf)]
  * Ive et al., (2019)
  * ACL'19
  * Previous work on multimodal machine translation has shown that visual information is only needed in very specific cases, for example in the presence of ambiguous words where the textual context is not sufficient. As a consequence, models tend to learn to ignore this information. We propose a translate-and-refine approach to this problem where images are only used by a second stage detector. This approach is trained jointly to generate a good first draft translation and to improve over this draft by (i) making better use of the target language textual context (both left and right-side contexts) and (ii) making use of visual context. This approach leads to the state of the art results. Additionally, we show that it has the ability to recover from erroneous or missing words in the source language.


## Datasets

* Flickr30K [[pdf](https://www.aclweb.org/anthology/Q14-1006)] [[web](http://shannon.cs.illinois.edu/DenotationGraph/)]
  * From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
  * Young et al., (2014)
  * We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.

* Flickr30K Entities [[pdf](https://arxiv.org/pdf/1505.04870.pdf)][[web](http://bryanplummer.com/Flickr30kEntities/)] [[github](https://github.com/BryanPlummer/flickr30k_entities)]
  * Flickr30K Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
  * Plummer et al., (2016)
  * The Flickr30K dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30K Entities, which augments the 158k captions from Flickr30K with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.

* Multi30K [[pdf](https://arxiv.org/pdf/1605.00459.pdf)] [[github](https://github.com/multi30k/dataset)]
  * Multi30K: Multilingual English-German Image Descriptions
  * Elliott et al., (2016)
  * We introduce the **Multi30K** dataset to simulate multilingual multimodal research. Recent advances in image description have been demonstrated on English-language datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flicker30K dataset with i) German translations created by professional translators over a subset of the English descriptions. We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks.